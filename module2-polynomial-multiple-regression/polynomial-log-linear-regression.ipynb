{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Lambda School Data Science_\n",
    "\n",
    "# Polynomial & Log-Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1o9Qe8ilN19"
   },
   "source": [
    "## \"Linear\" Regression?\n",
    "\n",
    "Which of the following is a linear regression model?\n",
    "\n",
    "![Functional Form Misspecification](http://www.ryanleeallred.com/wp-content/uploads/2018/08/functional-form-misspecification.jpg)\n",
    "\n",
    "**All** of these functional forms can be fit using Linear Regression. The \"Linear\" in linear regression refers to the linear form of the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4a5raiR9doDx"
   },
   "source": [
    "### Linear Combinations\n",
    "\n",
    "Remember when we rewrote vectors as a **linear combination** of scalars and unit vectors?\n",
    "\n",
    "\\begin{align}\n",
    "v = \\begin{bmatrix}2 \\\\ 3\\end{bmatrix} = 2 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + 3 \\begin{bmatrix} 0 \\\\ 1\\end{bmatrix} = 2\\hat{i} + 3\\hat{j} \n",
    "\\end{align}\n",
    "\n",
    "The syntax where we have a scalar (think coefficient) multiplying some vector (unit vector in this case) and all of them being added together is what makes this a linear combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gu-5ZGkFdpCS"
   },
   "source": [
    "### Linear Equations\n",
    "\n",
    "A \"Linear Equation\" is any equation that takes the following form: \n",
    "\n",
    "\\begin{align}\n",
    "a_1x_1 + \\ldots + a_nx_n + b = 0\n",
    "\\end{align}\n",
    "\n",
    "Does this look familiar? A linear equation is one where we have $x_1, \\ldots, x_n$ unknowns and $b, a_1, \\ldots, a_n$ coefficients which are considered parameters of the equation. \"The solutions of such an equation are the values that, when substituted to the unknowns, make the equality true.\"\n",
    "\n",
    "[Linear Equation Wikipedia](https://en.wikipedia.org/wiki/Linear_equation)\n",
    "\n",
    "Linear Regression is **linear** not because it can only plot straight lines and fit straight-line patterns in data, but because the form of the equation used to represent our regression is in the form of a **Linear Equation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUMyYQSM_w-9"
   },
   "source": [
    "### _So how do we fit curved data?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example #1: Moore's Law dataset\n",
    "\n",
    "#### Background\n",
    "- https://en.wikipedia.org/wiki/Moore%27s_law\n",
    "- https://en.wikipedia.org/wiki/Transistor_count\n",
    "\n",
    "#### Scrape HTML tables with Pandas!\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html\n",
    "- https://medium.com/@ageitgey/quick-tip-the-easiest-way-to-grab-data-out-of-a-web-page-in-python-7153cecfca58\n",
    "\n",
    "#### More web scraping options\n",
    "- https://automatetheboringstuff.com/chapter11/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3K1aHn-3BdD"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(139, 6), (6, 6), (2, 5), (80, 6), (13, 7), (27, 3), (0, 2), (15, 7), (46, 14), (0, 2), (3, 2), (2, 2), (2, 2), (5, 2), (7, 2)]\n"
     ]
    }
   ],
   "source": [
    "tables = pd.read_html('https://en.wikipedia.org/wiki/Transistor_count', header=0)\n",
    "print([table.shape for table in tables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processor</th>\n",
       "      <th>Transistor count</th>\n",
       "      <th>Date of introduction</th>\n",
       "      <th>Designer</th>\n",
       "      <th>Process</th>\n",
       "      <th>Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intel 4004</td>\n",
       "      <td>2300</td>\n",
       "      <td>1971</td>\n",
       "      <td>Intel</td>\n",
       "      <td>10,000 nm</td>\n",
       "      <td>12 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Intel 8008</td>\n",
       "      <td>3500</td>\n",
       "      <td>1972</td>\n",
       "      <td>Intel</td>\n",
       "      <td>10,000 nm</td>\n",
       "      <td>14 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MOS Technology 6502</td>\n",
       "      <td>3,510[3]</td>\n",
       "      <td>1975</td>\n",
       "      <td>MOS Technology</td>\n",
       "      <td>8,000 nm</td>\n",
       "      <td>21 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Motorola 6800</td>\n",
       "      <td>4100</td>\n",
       "      <td>1974</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>6,000 nm</td>\n",
       "      <td>16 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Intel 8080</td>\n",
       "      <td>4500</td>\n",
       "      <td>1974</td>\n",
       "      <td>Intel</td>\n",
       "      <td>6,000 nm</td>\n",
       "      <td>20 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RCA 1802</td>\n",
       "      <td>5000</td>\n",
       "      <td>1974</td>\n",
       "      <td>RCA</td>\n",
       "      <td>5,000 nm</td>\n",
       "      <td>27 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TMS 1000</td>\n",
       "      <td>8000</td>\n",
       "      <td>1974[4]</td>\n",
       "      <td>Texas Instruments</td>\n",
       "      <td>8,000 nm</td>\n",
       "      <td>11 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Intel 8085</td>\n",
       "      <td>6500</td>\n",
       "      <td>1976</td>\n",
       "      <td>Intel</td>\n",
       "      <td>3,000 nm</td>\n",
       "      <td>20 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Zilog Z80</td>\n",
       "      <td>8500</td>\n",
       "      <td>1976</td>\n",
       "      <td>Zilog</td>\n",
       "      <td>4,000 nm</td>\n",
       "      <td>18 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motorola 6809</td>\n",
       "      <td>9000</td>\n",
       "      <td>1978</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>5,000 nm</td>\n",
       "      <td>21 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Intel 8086</td>\n",
       "      <td>29000</td>\n",
       "      <td>1978</td>\n",
       "      <td>Intel</td>\n",
       "      <td>3,000 nm</td>\n",
       "      <td>33 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Intel 8088</td>\n",
       "      <td>29000</td>\n",
       "      <td>1979</td>\n",
       "      <td>Intel</td>\n",
       "      <td>3,000 nm</td>\n",
       "      <td>33 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WDC 65C02</td>\n",
       "      <td>11,500[5]</td>\n",
       "      <td>1981</td>\n",
       "      <td>WDC</td>\n",
       "      <td>3,000 nm</td>\n",
       "      <td>6 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Intel 80186</td>\n",
       "      <td>55000</td>\n",
       "      <td>1982</td>\n",
       "      <td>Intel</td>\n",
       "      <td>3,000 nm</td>\n",
       "      <td>60 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Motorola 68000</td>\n",
       "      <td>68,000[citation needed]</td>\n",
       "      <td>1979</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>3,500 nm</td>\n",
       "      <td>44 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Intel 80286</td>\n",
       "      <td>134000</td>\n",
       "      <td>1982</td>\n",
       "      <td>Intel</td>\n",
       "      <td>1,500 nm</td>\n",
       "      <td>49 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WDC 65C816</td>\n",
       "      <td>22,000[6]</td>\n",
       "      <td>1983</td>\n",
       "      <td>WDC</td>\n",
       "      <td>3000 nm[7]</td>\n",
       "      <td>9 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Motorola 68020</td>\n",
       "      <td>190,000[8]</td>\n",
       "      <td>1984</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>2,000 nm</td>\n",
       "      <td>85 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Intel 80386</td>\n",
       "      <td>275000</td>\n",
       "      <td>1985</td>\n",
       "      <td>Intel</td>\n",
       "      <td>1,500 nm</td>\n",
       "      <td>104 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ARM 1</td>\n",
       "      <td>25,000[8]</td>\n",
       "      <td>1985</td>\n",
       "      <td>Acorn</td>\n",
       "      <td>3,000 nm</td>\n",
       "      <td>50 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Novix NC4016</td>\n",
       "      <td>16,000[9]</td>\n",
       "      <td>1985[10]</td>\n",
       "      <td>Harris Corporation</td>\n",
       "      <td>3,000 nm[11]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ARM 2</td>\n",
       "      <td>30,000[8]</td>\n",
       "      <td>1986</td>\n",
       "      <td>Acorn</td>\n",
       "      <td>2,000 nm</td>\n",
       "      <td>30 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Motorola 68030</td>\n",
       "      <td>273000</td>\n",
       "      <td>1987</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>800 nm</td>\n",
       "      <td>102 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TI Explorer's 32-bit Lisp machine chip</td>\n",
       "      <td>553,000[12]</td>\n",
       "      <td>1987</td>\n",
       "      <td>Texas Instruments</td>\n",
       "      <td>2,000 nm[13]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DEC WRL MultiTitan</td>\n",
       "      <td>180,000[14]</td>\n",
       "      <td>1988</td>\n",
       "      <td>DEC WRL</td>\n",
       "      <td>1,500 nm</td>\n",
       "      <td>61 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Intel i960</td>\n",
       "      <td>250,000[15]</td>\n",
       "      <td>1988</td>\n",
       "      <td>Intel</td>\n",
       "      <td>600 nm</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Intel 80486</td>\n",
       "      <td>1180235</td>\n",
       "      <td>1989</td>\n",
       "      <td>Intel</td>\n",
       "      <td>1000 nm</td>\n",
       "      <td>173 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ARM 3</td>\n",
       "      <td>310000</td>\n",
       "      <td>1989</td>\n",
       "      <td>Acorn</td>\n",
       "      <td>1,500 nm</td>\n",
       "      <td>87 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>68040</td>\n",
       "      <td>1200000</td>\n",
       "      <td>1990</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>650 nm</td>\n",
       "      <td>152 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>R4000</td>\n",
       "      <td>1350000</td>\n",
       "      <td>1991</td>\n",
       "      <td>MIPS</td>\n",
       "      <td>1,000 nm</td>\n",
       "      <td>213 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>IBM z13</td>\n",
       "      <td>3990000000</td>\n",
       "      <td>2015</td>\n",
       "      <td>IBM</td>\n",
       "      <td>22 nm</td>\n",
       "      <td>678 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>12-core POWER8</td>\n",
       "      <td>4200000000</td>\n",
       "      <td>2013</td>\n",
       "      <td>IBM</td>\n",
       "      <td>22 nm</td>\n",
       "      <td>650 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Apple A11 Bionic (hexa-core ARM64 \"mobile SoC\")</td>\n",
       "      <td>4300000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>Apple</td>\n",
       "      <td>10 nm</td>\n",
       "      <td>89.23 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>15-core Xeon Ivy Bridge-EX</td>\n",
       "      <td>4,310,000,000[42]</td>\n",
       "      <td>2014</td>\n",
       "      <td>Intel</td>\n",
       "      <td>22 nm</td>\n",
       "      <td>541 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Zeppelin SoC Ryzen</td>\n",
       "      <td>4,800,000,000[43]</td>\n",
       "      <td>2017</td>\n",
       "      <td>AMD</td>\n",
       "      <td>14 nm</td>\n",
       "      <td>192 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Ryzen 5 1600 Ryzen</td>\n",
       "      <td>4,800,000,000[44]</td>\n",
       "      <td>2017</td>\n",
       "      <td>AMD</td>\n",
       "      <td>14 nm</td>\n",
       "      <td>213 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Ryzen 5 1600 X Ryzen</td>\n",
       "      <td>4,800,000,000[45]</td>\n",
       "      <td>2017</td>\n",
       "      <td>AMD</td>\n",
       "      <td>14 nm</td>\n",
       "      <td>213 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>61-core Xeon Phi</td>\n",
       "      <td>5,000,000,000[46]</td>\n",
       "      <td>2012</td>\n",
       "      <td>Intel</td>\n",
       "      <td>22 nm</td>\n",
       "      <td>720 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Xbox One main SoC</td>\n",
       "      <td>5000000000</td>\n",
       "      <td>2013</td>\n",
       "      <td>Microsoft/AMD</td>\n",
       "      <td>28 nm</td>\n",
       "      <td>363 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>18-core Xeon Haswell-E5</td>\n",
       "      <td>5,560,000,000[47]</td>\n",
       "      <td>2014</td>\n",
       "      <td>Intel</td>\n",
       "      <td>22 nm</td>\n",
       "      <td>661 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>IBM z14</td>\n",
       "      <td>6100000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>IBM</td>\n",
       "      <td>14 nm</td>\n",
       "      <td>696 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Apple A12 Bionic (hexa-core ARM64 \"mobile SoC\")</td>\n",
       "      <td>6,900,000,000[48][49]</td>\n",
       "      <td>2018</td>\n",
       "      <td>Apple</td>\n",
       "      <td>7 nm</td>\n",
       "      <td>83.27 mm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>HiSilicon Kirin 960 (octa-core ARM64 \"mobile S...</td>\n",
       "      <td>4,000,000,000[50]</td>\n",
       "      <td>2016</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>16 nm</td>\n",
       "      <td>110.00 mm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>HiSilicon Kirin 980 (octa-core ARM64 \"mobile S...</td>\n",
       "      <td>6,900,000,000[51]</td>\n",
       "      <td>2018</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>7 nm</td>\n",
       "      <td>74.13 mm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>HiSilicon Kirin 970 (octa-core ARM64 \"mobile S...</td>\n",
       "      <td>5,500,000,000[52]</td>\n",
       "      <td>2017</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>10 nm</td>\n",
       "      <td>96.72 mm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>HiSilicon Kirin 710 (octa-core ARM64 \"mobile S...</td>\n",
       "      <td>5,500,000,000[53]</td>\n",
       "      <td>2018</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>12 nm</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Xbox One X (Project Scorpio) main SoC</td>\n",
       "      <td>7,000,000,000[54]</td>\n",
       "      <td>2017</td>\n",
       "      <td>Microsoft/AMD</td>\n",
       "      <td>16 nm</td>\n",
       "      <td>360 mm²[54]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>IBM z13 Storage Controller</td>\n",
       "      <td>7100000000</td>\n",
       "      <td>2015</td>\n",
       "      <td>IBM</td>\n",
       "      <td>22 nm</td>\n",
       "      <td>678 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>28-core Xeon Platinum 8180</td>\n",
       "      <td>8,000,000,000[55]</td>\n",
       "      <td>2017</td>\n",
       "      <td>Intel</td>\n",
       "      <td>14 nm</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>22-core Xeon Broadwell-E5</td>\n",
       "      <td>7,200,000,000[56]</td>\n",
       "      <td>2016</td>\n",
       "      <td>Intel</td>\n",
       "      <td>14 nm</td>\n",
       "      <td>456 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>POWER9</td>\n",
       "      <td>8000000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>IBM</td>\n",
       "      <td>14 nm</td>\n",
       "      <td>695 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>72-core Xeon Phi</td>\n",
       "      <td>8000000000</td>\n",
       "      <td>2016</td>\n",
       "      <td>Intel</td>\n",
       "      <td>14 nm</td>\n",
       "      <td>683 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>IBM z14 Storage Controller</td>\n",
       "      <td>9700000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>IBM</td>\n",
       "      <td>14 nm</td>\n",
       "      <td>696 mm²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>32-core SPARC M7</td>\n",
       "      <td>10,000,000,000[57]</td>\n",
       "      <td>2015</td>\n",
       "      <td>Oracle</td>\n",
       "      <td>20 nm</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Apple A12X Bionic (octa-core ARM64 \"mobile SoC\")</td>\n",
       "      <td>10,000,000,000[58]</td>\n",
       "      <td>2018</td>\n",
       "      <td>Apple</td>\n",
       "      <td>7 nm</td>\n",
       "      <td>122 mm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Apple A10X Fusion (hexa-core ARM64 \"mobile SoC\")</td>\n",
       "      <td>4,300,000,000[59]</td>\n",
       "      <td>2017</td>\n",
       "      <td>Apple</td>\n",
       "      <td>10 nm</td>\n",
       "      <td>96.40 mm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Centriq 2400</td>\n",
       "      <td>18,000,000,000[60]</td>\n",
       "      <td>2017</td>\n",
       "      <td>Qualcomm</td>\n",
       "      <td>10 nm</td>\n",
       "      <td>398 mm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>32-core AMD Epyc</td>\n",
       "      <td>19200000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>AMD</td>\n",
       "      <td>14 nm</td>\n",
       "      <td>768 mm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>GC2 IPU</td>\n",
       "      <td>23600000000</td>\n",
       "      <td>2018</td>\n",
       "      <td>Graphcore</td>\n",
       "      <td>16 nm</td>\n",
       "      <td>825 mm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Tegra Xavier SoC</td>\n",
       "      <td>9,000,000,000[61]</td>\n",
       "      <td>2018</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>12 nm</td>\n",
       "      <td>350 mm²</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Processor  \\\n",
       "0                                           Intel 4004   \n",
       "1                                           Intel 8008   \n",
       "2                                  MOS Technology 6502   \n",
       "3                                        Motorola 6800   \n",
       "4                                           Intel 8080   \n",
       "5                                             RCA 1802   \n",
       "6                                             TMS 1000   \n",
       "7                                           Intel 8085   \n",
       "8                                            Zilog Z80   \n",
       "9                                        Motorola 6809   \n",
       "10                                          Intel 8086   \n",
       "11                                          Intel 8088   \n",
       "12                                           WDC 65C02   \n",
       "13                                         Intel 80186   \n",
       "14                                      Motorola 68000   \n",
       "15                                         Intel 80286   \n",
       "16                                          WDC 65C816   \n",
       "17                                      Motorola 68020   \n",
       "18                                         Intel 80386   \n",
       "19                                               ARM 1   \n",
       "20                                        Novix NC4016   \n",
       "21                                               ARM 2   \n",
       "22                                      Motorola 68030   \n",
       "23              TI Explorer's 32-bit Lisp machine chip   \n",
       "24                                  DEC WRL MultiTitan   \n",
       "25                                          Intel i960   \n",
       "26                                         Intel 80486   \n",
       "27                                               ARM 3   \n",
       "28                                               68040   \n",
       "29                                               R4000   \n",
       "..                                                 ...   \n",
       "109                                            IBM z13   \n",
       "110                                     12-core POWER8   \n",
       "111    Apple A11 Bionic (hexa-core ARM64 \"mobile SoC\")   \n",
       "112                         15-core Xeon Ivy Bridge-EX   \n",
       "113                                 Zeppelin SoC Ryzen   \n",
       "114                                 Ryzen 5 1600 Ryzen   \n",
       "115                               Ryzen 5 1600 X Ryzen   \n",
       "116                                   61-core Xeon Phi   \n",
       "117                                  Xbox One main SoC   \n",
       "118                            18-core Xeon Haswell-E5   \n",
       "119                                            IBM z14   \n",
       "120    Apple A12 Bionic (hexa-core ARM64 \"mobile SoC\")   \n",
       "121  HiSilicon Kirin 960 (octa-core ARM64 \"mobile S...   \n",
       "122  HiSilicon Kirin 980 (octa-core ARM64 \"mobile S...   \n",
       "123  HiSilicon Kirin 970 (octa-core ARM64 \"mobile S...   \n",
       "124  HiSilicon Kirin 710 (octa-core ARM64 \"mobile S...   \n",
       "125              Xbox One X (Project Scorpio) main SoC   \n",
       "126                         IBM z13 Storage Controller   \n",
       "127                         28-core Xeon Platinum 8180   \n",
       "128                          22-core Xeon Broadwell-E5   \n",
       "129                                             POWER9   \n",
       "130                                   72-core Xeon Phi   \n",
       "131                         IBM z14 Storage Controller   \n",
       "132                                   32-core SPARC M7   \n",
       "133   Apple A12X Bionic (octa-core ARM64 \"mobile SoC\")   \n",
       "134   Apple A10X Fusion (hexa-core ARM64 \"mobile SoC\")   \n",
       "135                                       Centriq 2400   \n",
       "136                                   32-core AMD Epyc   \n",
       "137                                            GC2 IPU   \n",
       "138                                   Tegra Xavier SoC   \n",
       "\n",
       "            Transistor count Date of introduction            Designer  \\\n",
       "0                       2300                 1971               Intel   \n",
       "1                       3500                 1972               Intel   \n",
       "2                   3,510[3]                 1975      MOS Technology   \n",
       "3                       4100                 1974            Motorola   \n",
       "4                       4500                 1974               Intel   \n",
       "5                       5000                 1974                 RCA   \n",
       "6                       8000              1974[4]   Texas Instruments   \n",
       "7                       6500                 1976               Intel   \n",
       "8                       8500                 1976               Zilog   \n",
       "9                       9000                 1978            Motorola   \n",
       "10                     29000                 1978               Intel   \n",
       "11                     29000                 1979               Intel   \n",
       "12                 11,500[5]                 1981                 WDC   \n",
       "13                     55000                 1982               Intel   \n",
       "14   68,000[citation needed]                 1979            Motorola   \n",
       "15                    134000                 1982               Intel   \n",
       "16                 22,000[6]                 1983                 WDC   \n",
       "17                190,000[8]                 1984            Motorola   \n",
       "18                    275000                 1985               Intel   \n",
       "19                 25,000[8]                 1985               Acorn   \n",
       "20                 16,000[9]             1985[10]  Harris Corporation   \n",
       "21                 30,000[8]                 1986               Acorn   \n",
       "22                    273000                 1987            Motorola   \n",
       "23               553,000[12]                 1987   Texas Instruments   \n",
       "24               180,000[14]                 1988             DEC WRL   \n",
       "25               250,000[15]                 1988               Intel   \n",
       "26                   1180235                 1989               Intel   \n",
       "27                    310000                 1989               Acorn   \n",
       "28                   1200000                 1990            Motorola   \n",
       "29                   1350000                 1991                MIPS   \n",
       "..                       ...                  ...                 ...   \n",
       "109               3990000000                 2015                 IBM   \n",
       "110               4200000000                 2013                 IBM   \n",
       "111               4300000000                 2017               Apple   \n",
       "112        4,310,000,000[42]                 2014               Intel   \n",
       "113        4,800,000,000[43]                 2017                 AMD   \n",
       "114        4,800,000,000[44]                 2017                 AMD   \n",
       "115        4,800,000,000[45]                 2017                 AMD   \n",
       "116        5,000,000,000[46]                 2012               Intel   \n",
       "117               5000000000                 2013       Microsoft/AMD   \n",
       "118        5,560,000,000[47]                 2014               Intel   \n",
       "119               6100000000                 2017                 IBM   \n",
       "120    6,900,000,000[48][49]                 2018               Apple   \n",
       "121        4,000,000,000[50]                 2016              Huawei   \n",
       "122        6,900,000,000[51]                 2018              Huawei   \n",
       "123        5,500,000,000[52]                 2017              Huawei   \n",
       "124        5,500,000,000[53]                 2018              Huawei   \n",
       "125        7,000,000,000[54]                 2017       Microsoft/AMD   \n",
       "126               7100000000                 2015                 IBM   \n",
       "127        8,000,000,000[55]                 2017               Intel   \n",
       "128        7,200,000,000[56]                 2016               Intel   \n",
       "129               8000000000                 2017                 IBM   \n",
       "130               8000000000                 2016               Intel   \n",
       "131               9700000000                 2017                 IBM   \n",
       "132       10,000,000,000[57]                 2015              Oracle   \n",
       "133       10,000,000,000[58]                 2018               Apple   \n",
       "134        4,300,000,000[59]                 2017               Apple   \n",
       "135       18,000,000,000[60]                 2017            Qualcomm   \n",
       "136              19200000000                 2017                 AMD   \n",
       "137              23600000000                 2018           Graphcore   \n",
       "138        9,000,000,000[61]                 2018              Nvidia   \n",
       "\n",
       "          Process         Area  \n",
       "0       10,000 nm       12 mm²  \n",
       "1       10,000 nm       14 mm²  \n",
       "2        8,000 nm       21 mm²  \n",
       "3        6,000 nm       16 mm²  \n",
       "4        6,000 nm       20 mm²  \n",
       "5        5,000 nm       27 mm²  \n",
       "6        8,000 nm       11 mm²  \n",
       "7        3,000 nm       20 mm²  \n",
       "8        4,000 nm       18 mm²  \n",
       "9        5,000 nm       21 mm²  \n",
       "10       3,000 nm       33 mm²  \n",
       "11       3,000 nm       33 mm²  \n",
       "12       3,000 nm        6 mm²  \n",
       "13       3,000 nm       60 mm²  \n",
       "14       3,500 nm       44 mm²  \n",
       "15       1,500 nm       49 mm²  \n",
       "16     3000 nm[7]        9 mm²  \n",
       "17       2,000 nm       85 mm²  \n",
       "18       1,500 nm      104 mm²  \n",
       "19       3,000 nm       50 mm²  \n",
       "20   3,000 nm[11]          NaN  \n",
       "21       2,000 nm       30 mm²  \n",
       "22         800 nm      102 mm²  \n",
       "23   2,000 nm[13]          NaN  \n",
       "24       1,500 nm       61 mm²  \n",
       "25         600 nm          NaN  \n",
       "26        1000 nm      173 mm²  \n",
       "27       1,500 nm       87 mm²  \n",
       "28         650 nm      152 mm²  \n",
       "29       1,000 nm      213 mm²  \n",
       "..            ...          ...  \n",
       "109         22 nm      678 mm²  \n",
       "110         22 nm      650 mm²  \n",
       "111         10 nm    89.23 mm²  \n",
       "112         22 nm      541 mm²  \n",
       "113         14 nm      192 mm²  \n",
       "114         14 nm      213 mm²  \n",
       "115         14 nm      213 mm²  \n",
       "116         22 nm      720 mm²  \n",
       "117         28 nm      363 mm²  \n",
       "118         22 nm      661 mm²  \n",
       "119         14 nm      696 mm²  \n",
       "120          7 nm    83.27 mm2  \n",
       "121         16 nm   110.00 mm2  \n",
       "122          7 nm    74.13 mm2  \n",
       "123         10 nm    96.72 mm2  \n",
       "124         12 nm          NaN  \n",
       "125         16 nm  360 mm²[54]  \n",
       "126         22 nm      678 mm²  \n",
       "127         14 nm          NaN  \n",
       "128         14 nm      456 mm²  \n",
       "129         14 nm      695 mm²  \n",
       "130         14 nm      683 mm²  \n",
       "131         14 nm      696 mm²  \n",
       "132         20 nm          NaN  \n",
       "133          7 nm      122 mm2  \n",
       "134         10 nm    96.40 mm2  \n",
       "135         10 nm      398 mm2  \n",
       "136         14 nm      768 mm2  \n",
       "137         16 nm      825 mm2  \n",
       "138         12 nm      350 mm²  \n",
       "\n",
       "[139 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moore = tables[0]\n",
    "moore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 139 entries, 0 to 138\n",
      "Data columns (total 6 columns):\n",
      "Processor               139 non-null object\n",
      "Transistor count        135 non-null object\n",
      "Date of introduction    139 non-null object\n",
      "Designer                139 non-null object\n",
      "Process                 139 non-null object\n",
      "Area                    130 non-null object\n",
      "dtypes: object(6)\n",
      "memory usage: 6.6+ KB\n"
     ]
    }
   ],
   "source": [
    "moore.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-caf60dbae474>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                      \u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'['\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Remove citations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                      \u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\D'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Remove non-digit characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                      .astype(int))\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmoore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmoore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors, **kwargs)\u001b[0m\n\u001b[0;32m   4999\u001b[0m             \u001b[1;31m# else, only a single dtype is given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5000\u001b[0m             new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors,\n\u001b[1;32m-> 5001\u001b[1;33m                                          **kwargs)\n\u001b[0m\u001b[0;32m   5002\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[0;32m   3712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3713\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3714\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'astype'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3716\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m   3579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3580\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mgr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3581\u001b[1;33m             \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3582\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'raise'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m         return self._astype(dtype, copy=copy, errors=errors, values=values,\n\u001b[1;32m--> 575\u001b[1;33m                             **kwargs)\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     def _astype(self, dtype, copy=False, errors='raise', values=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_astype\u001b[1;34m(self, dtype, copy, errors, values, klass, mgr, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m                 \u001b[1;31m# _astype_nansafe works fine with 1-d only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[1;31m# work around NumPy brokenness, #1987\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype_intsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m         \u001b[1;31m# if we have a datetime/timedelta array of objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.astype_intsafe\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\util.pxd\u001b[0m in \u001b[0;36mutil.set_value_at_unsafe\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "feature = 'Date of introduction'\n",
    "target  = 'Transistor count'\n",
    "\n",
    "moore = moore.dropna(subset=[feature, target]).copy()\n",
    "\n",
    "for column in [feature, target]:\n",
    "    moore[column] = (moore[column]\n",
    "                     .str.split('[').str[0]  # Remove citations\n",
    "                     .str.replace(r'\\D','')  # Remove non-digit characters\n",
    "                     .astype(int))\n",
    "    \n",
    "moore = moore.sort_values(by=feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(moore[target]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moore.plot(x=feature, y=target, kind='scatter', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moore.plot(x=feature, y=target, kind='scatter', alpha=0.5, logy=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = moore.plot(x=feature, y=target, kind='scatter', alpha=0.5)\n",
    "\n",
    "X = moore[[feature]]\n",
    "y = moore[target]\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "ax.plot(X, model.predict(X))\n",
    "print('R^2', model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = moore.plot(x=feature, y=target, kind='scatter', alpha=0.5, logy=True)\n",
    "\n",
    "X = moore[[feature]]\n",
    "y = np.log(moore[target]) # Apply natural log function to the target\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = np.exp(model.predict(X)) # Apply exponential function (inverse of natural log) to the predictions\n",
    "ax.plot(X, y_pred) \n",
    "print('R^2', model.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = moore.plot(x=feature, y=target, kind='scatter', alpha=0.5)\n",
    "ax.plot(X, y_pred)\n",
    "print('R^2', model.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make polynomial feature\n",
    "moore['Date of introduction ** 2'] = moore['Date of introduction'] ** 2\n",
    "\n",
    "features = ['Date of introduction', 'Date of introduction ** 2']\n",
    "X = moore[features]\n",
    "y = moore[target]\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "ax = moore.plot(x='Date of introduction', y=target, kind='scatter', alpha=0.5)\n",
    "ax.plot(X['Date of introduction'], model.predict(X))\n",
    "print('R^2', model.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize for higher degree polynomials, and print equation\n",
    "\n",
    "def polynomial_regression(degrees=2):\n",
    "    \n",
    "    # Make polynomial features\n",
    "    feature = 'Date of introduction'\n",
    "    polynomial_features = []\n",
    "    for degree in range(2, degrees+1):\n",
    "        name = f'{feature} ** {degree}'\n",
    "        moore[name] = moore[feature] ** degree\n",
    "        polynomial_features.append(name)\n",
    "    \n",
    "    features = [feature] + polynomial_features\n",
    "    target  = 'Transistor count'\n",
    "    X = moore[features]\n",
    "    y = moore[target]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = moore.plot(x=feature, y=target, kind='scatter', alpha=0.5)\n",
    "    ax.plot(moore[feature], model.predict(X))\n",
    "    betas = [model.intercept_] + model.coef_.tolist()\n",
    "    equation = ' + '.join(f'{beta}x**{i}' for i, beta in enumerate(betas))\n",
    "    print(equation)\n",
    "    print('R^2', model.score(X, y))\n",
    "    \n",
    "polynomial_regression(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(polynomial_regression, degrees=(1,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_cs8PskIdy9t"
   },
   "source": [
    "# Polynomial Regression, explained\n",
    "\n",
    "Just as multiple regression was an extension of the bivariate case, Polynomial Regression is an extention of multiple regression and can be used to fit data to any (curved) shape. This is one of the reasons why data exploration is so important. You won't know that you need to fit a polynomial function to a feature unless you have examined its distribution.\n",
    "\n",
    "[Why is polynomial regression considered a special case of multiple linear regression?](https://stats.stackexchange.com/questions/92065/why-is-polynomial-regression-considered-a-special-case-of-multiple-linear-regres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptmg8FRty5Pu"
   },
   "source": [
    "# Example #1: King County Housing Data\n",
    "\n",
    "[from Kaggle](https://www.kaggle.com/harlfoxem/housesalesprediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1840,
     "status": "ok",
     "timestamp": 1550599140263,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "9dEQTcwHy8mY",
    "outputId": "7928f778-38e2-4a74-f2df-53e32cc4764e"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/kc_house_data.csv')\n",
    "print(df.shape)\n",
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCwbKGIOzqbO"
   },
   "source": [
    "## Find a \"curved\" feature in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'price'\n",
    "numeric_columns = df.select_dtypes(include='number').columns\n",
    "for feature in numeric_columns.drop(target):\n",
    "    sns.scatterplot(x=feature, y=target, data=df, alpha=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H6X-TDuL2PaG"
   },
   "source": [
    "## Make or \"engineer\" a new grade_squared feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16806,
     "status": "ok",
     "timestamp": 1550599156323,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "lKiJSmUn2Ohr",
    "outputId": "c8a41385-5613-403c-e193-cf91d31febf3"
   },
   "outputs": [],
   "source": [
    "df['grade_squared'] = df['grade']**2\n",
    "for feature in ['grade', 'grade_squared']:\n",
    "    sns.scatterplot(x=feature, y=target, data=df, alpha=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PTdqhfQY1hLb"
   },
   "source": [
    "## Test the fit of a polynomial regression to that feature\n",
    "\n",
    "First we'll fit a regular bivariate regression line and calculate its $R^2$ to get a baseline. Since we want to know if this generated feature is improving our model or not we'll first run our code without it so that we have something to compare to. \n",
    "\n",
    "$price_i = \\beta_0 + \\beta_1grade_i + \\epsilon_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32552,
     "status": "ok",
     "timestamp": 1550599172729,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "l5eFe55l2JHT",
    "outputId": "578adb74-904b-49c7-ddd2-d7c1600f637f"
   },
   "outputs": [],
   "source": [
    "# Separate dependent and independent variables\n",
    "target = 'price'\n",
    "features = ['grade']\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "\n",
    "def run_linear_model(X, y):\n",
    "    # Split into test and train data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Fit model using train data\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions using test features\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compare predictions to test target\n",
    "    rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print('Root Mean Squared Error', rmse)\n",
    "    print('R^2 Score', r2)\n",
    "    print('Intercept', model.intercept_)\n",
    "    coefficients = pd.Series(model.coef_, X_train.columns)\n",
    "    print(coefficients.to_string())\n",
    "    \n",
    "run_linear_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4dAZb0U4-vE"
   },
   "source": [
    "## Lets try fitting grade_squared as a bivariate model\n",
    "\n",
    "$price_i = \\beta_0 + \\beta_1grade^{2}_i + \\epsilon_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32093,
     "status": "ok",
     "timestamp": 1550599172730,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "Nfe7mTsm4-Bv",
    "outputId": "2682e202-bfda-4c81-bded-3dff017e592a"
   },
   "outputs": [],
   "source": [
    "# Separate dependent and independent variables\n",
    "target = 'price'\n",
    "features = ['grade_squared']\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "\n",
    "run_linear_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9lSw_KYF5wSB"
   },
   "source": [
    "## Multiple Regression using both grade and grade_squred\n",
    "\n",
    "$price_i = \\beta_0 + \\beta_1 grade_i + \\beta_2grade^{2}_i + \\epsilon_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31694,
     "status": "ok",
     "timestamp": 1550599172732,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "avqSfsJr2h2I",
    "outputId": "0d932f23-6c1b-417f-e7ee-8372b4d2561e"
   },
   "outputs": [],
   "source": [
    "# Separate dependent and independent variables\n",
    "target = 'price'\n",
    "features = ['grade', 'grade_squared']\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "\n",
    "run_linear_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "loZ378cZ7mIC"
   },
   "source": [
    "# How to find non-linear features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fM6fKuubBy8b"
   },
   "source": [
    "## 1) Domain Knowledge (think about your variables and hypothesize)\n",
    "\n",
    "This is why having domain knowledge about the problem that you're trying to solve is something that's so important. In the context of home prices, variables that have a curved structure often are that way due to some form of diminishing returns increases in certain amenities. Lets think about the following variables:\n",
    "\n",
    "- Lot Size:\n",
    "\n",
    "The more land you're willing to buy all at once the cheaper it will be on a per-acre basis (Saving money when you buy in bulk). This trend carries through to small to medium sized lots but but with a more shallow curve.\n",
    "\n",
    "![Price Per Acre](https://placercountyhomesandland.typepad.com/photos/uncategorized/price_per_acre_graph.png)\n",
    "\n",
    "- Square Footage:\n",
    "\n",
    "Square footage of a home sees a similar pattern. The value an additional 100 square feet in small homes (imagine the difference between say a 800 sq foot home and a 900 sq foot home) makes a big difference to buyers, where as an additional 100 square feet in a mansion probably isn't valued quite as highly. \n",
    "\n",
    "- Age:\n",
    "\n",
    "Just like how the prices of new cars drop steeply in the first few years, the value of homes due to age drop quickly in the first few years after its built and then less quickly as time goes on. This is not a linear pattern and needs to be fitted by a polynomial model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5cBlL1xBzFw"
   },
   "source": [
    "## 2) Visual Inspection\n",
    "\n",
    "We already talked about how generating scatterplots or other graphs is vital in our data exploration stage and can lead us to identify possible candidates for polynomials. Here I just wanted to share one more tip that can help you analyze scatterplots when you have a lot of data.\n",
    "\n",
    "If you have so much data that it's hard to tell what's going on in your scatterplot, then sample your data and regenerate them to get a better idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39338,
     "status": "ok",
     "timestamp": 1550599181362,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "udIeJAY-B94U",
    "outputId": "9cec3697-7aa8-459f-8e39-7e8c2e2cc513"
   },
   "outputs": [],
   "source": [
    "# Sample our dataframe to take 1/20th the values\n",
    "sampled = df.sample(frac=0.05, replace=True, random_state=42)\n",
    "\n",
    "target = 'price'\n",
    "numeric_columns = df.select_dtypes(include='number').columns\n",
    "for feature in numeric_columns.drop(target):\n",
    "    sns.scatterplot(x=feature, y=target, data=sampled, alpha=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDyTStZgBzIU"
   },
   "source": [
    "## 3) Inspect the distribution of residuals\n",
    "\n",
    "![Poor Fit Residuals](http://www.thejavageek.com/wp-content/uploads/2018/02/linear-regression-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRF9aISzGRyr"
   },
   "source": [
    "## An Aside: The \"Hedonic Housing Model\"\n",
    "\n",
    "Using Linear Regression to model home prices is a very common use of predictive linear regression modeling. It's so common fact that it has its own name: The Hedonic Housing Model. In the Hedonic Housing model it is well understood and reiterated that certain features tend to be curved in nature and these polynomial features (like the ones mentioned above) have all been well explored in real estate prediction circles. This is another of how domain knowledge can give you an edge. The best way to gain domain knowledge is to dive in and try and solve one particular kind of problem, and pick up little tips and tidbits as time goes on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1xiwKaUP0Y_"
   },
   "source": [
    "# Log-Linear Regression\n",
    "\n",
    "In a log-linear regression model, we take the natural log of all of our y variable and use that as our y vector instead of the raw y values. Why would we do that?\n",
    "\n",
    "<https://www.kaggle.com/c/house-prices-advanced-regression-techniques/discussion/60861>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kVJde3zZrms"
   },
   "source": [
    "## 1) To reduce skew in y\n",
    "\n",
    "Where we have variables with lots of relatively low values and few high values (like with home prices) we would expect to see our data more clustered on the left-hand side with a long tail extending to the right up into the expensive homes. The fact of the matter is that we will be able to make better predictions if we can normalize our data to some degree and one way to do this is by taking the natural log of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rdS22S_OQ-vr"
   },
   "outputs": [],
   "source": [
    "df['ln_price'] = np.log(df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37701,
     "status": "ok",
     "timestamp": 1550599181593,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "YqmZNtUOTZoN",
    "outputId": "6125f6ed-2df1-404e-d977-32135f41bf00"
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['price']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37533,
     "status": "ok",
     "timestamp": 1550599181805,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "p7CDc9_ETeUf",
    "outputId": "cd9f3f3f-9936-4e2d-c3fb-2482ca04ddc9"
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['ln_price']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXrEJWb8akIL"
   },
   "source": [
    "## New distribution of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52845,
     "status": "ok",
     "timestamp": 1550599197819,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "whrf7MbDYisJ",
    "outputId": "beffe32c-c216-46fd-97f9-cd9619ad3955"
   },
   "outputs": [],
   "source": [
    "target = 'ln_price'\n",
    "numeric_columns = df.select_dtypes(include='number').columns\n",
    "for feature in numeric_columns.drop(target):\n",
    "    sns.scatterplot(x=feature, y=target, data=df, alpha=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sd4mICl9ZrvR"
   },
   "source": [
    "## 2) Make coefficients easier to interpret\n",
    "\n",
    "Transforming our price values in this way won't change our model's ability to generate predictions, but what it **will** do is change the interpretation of all of our x-coefficients. This will change our x-coefficients from have an elasticity type interpretation (a raw dollar amount change if there is a 1 unit increase in x) to having a percentage-terms interpretation. Lets demonstrate and talk about this further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-Yt7EcZZr33"
   },
   "source": [
    "## 3) Make our errors easier to interpret\n",
    "\n",
    "Errors that have been calculated on variables that are in log form can also be interpreted roughly as percentage error. We've been using percentages all our lives and they have immediate meaning to us. This is why I prefer log-linear regression models when possible.\n",
    "\n",
    "<https://people.duke.edu/~rnau/411log.htm>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f1ecoaG9K-FB"
   },
   "source": [
    "Feature Engineering is a big topic in machine learning. We won't be able to cover every aspect of it today, but hopefully we can give you a strong idea of what it is and how to go about it. \n",
    "\n",
    "[Understanding Feature Engineering Part 1](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)\n",
    "\n",
    "Feature engineering is key to success in predictive modeling. It is the process by which we take existing features and combine them or alter them in ways that will expose additional signal to our model. Feature engineering is all about making the most of the data that we already had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1550599291855,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "gLC6Zz1pQaef",
    "outputId": "11b76865-0d19-4617-8047-6c114f809f0a"
   },
   "outputs": [],
   "source": [
    "# Log-Linear Regression\n",
    "# Separate dependent and independent variables\n",
    "target = 'ln_price'\n",
    "features = ['grade']\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "\n",
    "run_linear_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwlCapGPUwFe"
   },
   "source": [
    "This means that a one unit increase in the grade of a home increases its sale price by 31%. Often it is much easier to interpret coefficients in this manner than in the regular way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50803,
     "status": "ok",
     "timestamp": 1550599197821,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "ZVNHMgpuXo5z",
    "outputId": "460e0b13-54ae-4ae9-82d3-09078d3007ee"
   },
   "outputs": [],
   "source": [
    "## Log-Linear Regression\n",
    "# Separate dependent and independent variables\n",
    "target = 'ln_price'\n",
    "features = ['grade', 'grade_squared']\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "\n",
    "run_linear_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCdxF6o1Rowb"
   },
   "source": [
    "Our RMSE is really \"small\" now because it now represents error in percentage terms. We're on average about 37% off in our predictions of house prices.\n",
    "\n",
    "Our coefficients can also be understood in percentage terms which makes the coefficients on our regression much more digestable at a glance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5TODtjOJZzr"
   },
   "source": [
    "## A note about $R^2$\n",
    "\n",
    "$R^2$ If we add any feature to our model (even nearly meaningless ones) our $R^2$ will improve. For this reason raw $R^2$ is not the ultimate measure of goodness of fit. It is informative but completely secondary to our Root-Mean-Squared-Error (in predictive modeling). While a higher $R^2$ is generally better, this isn't the thing that we're trying to optimize. We care more about minimizing RMSE than maximizing $R^2$\n",
    "\n",
    "### \"Kitchen Sink\" models\n",
    "\n",
    "You may hear the term \"kitchen sink\" regression model used. This refers to a regression model that throws every available explanatory variable into the model in an attempt to improve it without much thought to whether those variables should really be considered as affecting y. Kitchen Sink models will have a higher $R^2$ than others but will have higher standard errors (estimates about particular coefficients may be less precise). \n",
    "\n",
    "Therefore, you tend to see \"Kitchen Sink\" models when the only priority is predictive accuracy and not interpretability. \n",
    "\n",
    "### Alternative measures of Goodness-of-fit\n",
    "\n",
    "Efforts have been made to improve upon $R^2$ due to these limitations. A metric called \"Adjusted $R^2$\" seeks to account for the number of explanatory variables included in a model and adjust $R^2$ accordingly. This is something that you can look up if you're curious. I just wanted to make you aware of it more than anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRh1C4qJXWY3"
   },
   "source": [
    "## A note about dirty linear regression data\n",
    "\n",
    "### Linear Regression models can only process numeric values\n",
    "\n",
    "### Your data must be free of NaN values before being passed to the algorithm \n",
    "\n",
    "(some data cleaning will be required in today's assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "36wffNrwZJ6V"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4B98U_YoMBTY"
   },
   "source": [
    "### Polynomial Regression requires feature engineering\n",
    "\n",
    "You've already seen an example of feature engineering today when we created the $grade^2$ variable. We took an existing feature and used it to generate a new feature that exposed the data to the model in a slightly different way. \n",
    "\n",
    "\"Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\" - [Jason Brownlee](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)\n",
    "\n",
    "### What features could we engineer with the King County dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1547533732470,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "WKs8ww9yj3e3",
    "outputId": "9021a38b-cf80-4870-d9d0-a2d2e29ca2cc"
   },
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pk15gYRxMw9_"
   },
   "source": [
    "- **[date]** The date is in a format that is not super useful to us. If were to extract the year we could then take the difference between year and yr_built to find the age of the home. We could also include the squared term of the home age since we know (from our hedonic housing model domain knowledge) that home age typically is not linear. \n",
    "- **[bedrooms]** & **[bathrooms]** We maybe use a combined measure of bedrooms and bathrooms, or find an average room square footage by taking total number of rooms and dividing by the square footage. \n",
    "- **[sqft_living]** **[sqft_lot]** The difference between lot square footage and home square footage ought to also give us a rough measure of the size of the yard. Rough measures are fine as long as the engineered features expose some new shred of meaning to our model.\n",
    "- **[floors]** We could calculate an average number of square feet per floor\n",
    "- **[lat]** **[long]** There are all kinds of things that we could do with the latitude and longitude especially if we use some kind of outside API or external dataset to bring in new features associated with the location of the homes. This would take a lot of work but these could potentially be very powerful features.\n",
    "\n",
    "### Kaggle is one of the best places to get feature engineering ideas\n",
    "\n",
    "Look at Kernels: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/kernels\n",
    "\n",
    "Here's just one example: https://www.kaggle.com/thevachar/house-price-regression-and-feature-engineering\n",
    "\n",
    "Kaggle has definitions of the columns too: https://www.kaggle.com/harlfoxem/housesalesprediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with baseline, before feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1550602599055,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "ESFfcliEgt9y",
    "outputId": "31ef7d05-9969-442b-b294-b6a003fa7a25"
   },
   "outputs": [],
   "source": [
    "df['ln_price'] = np.log(df['price'])\n",
    "target = 'ln_price'\n",
    "numeric_columns = df.select_dtypes(include='number').columns\n",
    "features = numeric_columns.drop([target, 'price', 'id'])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGHxukhjHsWu"
   },
   "outputs": [],
   "source": [
    "# Non-Feature Engineered Baseline Model\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "run_linear_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer a new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZD0a0JyUInGu"
   },
   "outputs": [],
   "source": [
    "df['age'] = 2015 - df['yr_built']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include='number').columns\n",
    "features = numeric_columns.drop([target, 'price', 'id'])\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "run_linear_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you engineer more features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Nww7246e4pn"
   },
   "source": [
    "## More topics\n",
    "\n",
    "### Interaction Terms\n",
    "\n",
    "An interaction occurs when an independent variable has a different effect on the outcome depending on the values of another independent variable.\n",
    "\n",
    "Lets look at an example where we were trying to estimate the level of satisfaction that a person would have when eating some kind of food with a condiment (sauce) on it.\n",
    "\n",
    "$satisfaction_i = \\beta_0 + \\beta_1 food_i + \\beta_2condiment_i + \\epsilon$\n",
    "\n",
    "Imagine that we have two foods: Ice Cream and Hot Dogs, and we also have to condiments: hot fudge and mustard. \n",
    "\n",
    "$\\beta_1$ in this example is trying to capture the effect of on satisfaction between eating hot dogs vs eating ice cream, and $\\beta_2$ is trying to capture the effect of putting hot fudge (chocolate sauce) vs mustard on your food. \n",
    "\n",
    "$\\beta_2$ is a little more problematic in this scenario. If someone were to come up to you and ask if you preferred hot fudge or mustard on your food, how would you answer?\n",
    "\n",
    "You would probably say something like **\"IT DEPENDS ON WHAT THE FOOD IS.\"** This means that the effect of our x variables on y (satisfaction) depends on the combination of food and condiment. I don't know about you guys, but I wouldn't be as satisfied if I had hot fudge on my hot dog or mustard on my ice cream. \n",
    "\n",
    "An interaction terms is something that we add to our regression to account for these \"It Depends\" moments between two x variables. We do this by multiplying the two of them together or *interacting* them with each other to capture the implications of the different combinations taking place.\n",
    "\n",
    "$satisfaction_i = \\beta_0 + \\beta_1 food_i + \\beta_2condiment_i + \\beta_3(food\\times condiment_i) + \\epsilon$\n",
    "\n",
    "<http://statisticsbyjim.com/regression/interaction-effects/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ZH-5O0lLbJ7"
   },
   "source": [
    "### Removing Outliers\n",
    "\n",
    "To remove outliers via the 1.5*Interquartile-Range method. The first step is to calculate the IQR for each variable.\n",
    "\n",
    "The IQR is the difference between the 25th and 75th percentiles of the feature.\n",
    "\n",
    "Find the IQR and multiply it by 1.5\n",
    "\n",
    "Then add the 1.5*IQR to the 3rd quartile (75th percentile). Anything above that range is an outlier.\n",
    "Subtract 1.5*IQR from the 1st quartile (25th percentile). Anything below that value is also an outlier.\n",
    "\n",
    "You want to minimize outliers in your dataset, so remove them by dropping observations that contain outliers in key features.\n",
    "\n",
    "Typically you will wan to remove outliers before doing anything else with your dataset. We haven't focused on this strongly yet in the class, but coefficients get strongly biased by outliers so if you want to really have accurate predictions, remove outliers before you begin your feature engineering and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQ08zZkxW0hY"
   },
   "source": [
    "# Major Takeaways\n",
    "\n",
    "- Polynomial Regression\n",
    "  - Linear Regression can fit curved lines.\n",
    "  - Including squared and cubed terms can improve fit and accuracy.\n",
    "\n",
    "- Log-linear Regression\n",
    "  - ln(y) helps normalize data with a skewed y variable.\n",
    "  - ln(y) changes interpretation of coefficients and errors to be percentages.\n",
    "\n",
    "- Feature Engineering\n",
    "  - Generating Features improves model accuracy if done well.\n",
    "  - This is where creativity and domain knowledge really pay off.\n",
    "  - When you think that certain combinations of x variables might affect y differently than how they do separately. Include an interaction term."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Polynomial/Log-linear Regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
