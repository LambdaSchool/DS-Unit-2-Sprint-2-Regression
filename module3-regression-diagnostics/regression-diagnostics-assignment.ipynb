{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Diagnostics Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/will-cotton4/DS-Unit-2-Sprint-2-Regression/blob/master/module3-regression-diagnostics/regression-diagnostics-assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pTkoSVmLhAZx"
      },
      "cell_type": "markdown",
      "source": [
        "# Regression Diagnostics\n",
        "\n",
        "The purpose of this assigment is introduce you to a new library for linear regression called statmodels which is much better suited for inferential modeling than sklearn. This assignment is also to familiarize yourself with some of most important procedures for improving the interpretability of regression coefficients. You will also perform important statistical tests that will help establish that whether or not important assumptions that safeguard the interpretability of OLS coefficients have been met. \n",
        "\n",
        "We will continue to use the Ames Housing Dataset so that you can focus on the techniques and not on cleaning/getting associated with a brand new dataset."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yb24I_Y0iC4M"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Choose an X and Y variable from your dataset and use them to create a Seaborn Regplot"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "N0NCgQkHie-r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import statsmodels.api as sm\n",
        "from scipy import stats\n",
        "%matplotlib inline\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Km4EwAPI-juw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Stole this cleaning info from Josh. Thanks Josh.\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/Ames%20Housing%20Data/train.csv'\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "#columns to drop bc of NaNs:\n",
        "drop_cols = ['Alley', 'Fence', 'MiscFeature']\n",
        "df = df.drop(columns = drop_cols)\n",
        "\n",
        "#Fill Nans for garage yr blt w/ YearBlt value:\n",
        "nums = df[df.GarageYrBlt.isna() == True].index\n",
        "for i in nums:\n",
        "  df.at[i, 'GarageYrBlt'] = df.at[i, 'YearBuilt']\n",
        "  \n",
        "#drop 8 rows of masonry veneer area Nans:\n",
        "df.dropna(subset=['MasVnrArea'], inplace=True)\n",
        "\n",
        "#replace most of the categoricals with numbers:\n",
        "map = {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, np.NaN: 0}\n",
        "df = df.replace(map)\n",
        "\n",
        "#Fill NaNs for LotFrontage as 0:\n",
        "#df['LotFrontage'] = df['LotFrontage'].fillna(0)\n",
        "\n",
        "#create log price column\n",
        "df['ln_price'] = np.log(df['SalePrice'])\n",
        "\n",
        "#age:\n",
        "df['age'] = 2010 - df['YearBuilt']\n",
        "\n",
        "#renovation_age:\n",
        "df['renovation_age'] = 2010 - df['YearRemodAdd']\n",
        "\n",
        "#yard size (ignores deck, pool, etc):\n",
        "df['yard_size'] = df['LotArea'] - df['1stFlrSF']\n",
        "\n",
        "#garage finish:\n",
        "finish_map = {'Fin':3, 'RFn': 2, 'Unf':1, np.NaN:0}\n",
        "df['GarageFinish'] = df['GarageFinish'].replace(finish_map)\n",
        "\n",
        "#sale condition:\n",
        "sale_map = {'Normal': 3,\n",
        "            'Partial': 3,\n",
        "            'Abnorml': 1,\n",
        "            'Family': 1,\n",
        "            'AdjLand': 2,\t\n",
        "            'Alloca': 2}\n",
        "\n",
        "df['SaleCondition'] = df['SaleCondition'].replace(sale_map)\n",
        "\n",
        "#Lot shape:\n",
        "shape_map = {'Reg': 4, 'IR1': 3, 'IR2': 2, 'IR3': 1}\n",
        "df[\"LotShape\"] = df[\"LotShape\"].replace(shape_map)\n",
        "\n",
        "#Street:\n",
        "street_map = {'Grvl':1, 'Pave':2}\n",
        "df['Street'] = df['Street'].replace(street_map)\n",
        "\n",
        "#Neighborhood Quality:\n",
        "neighborhood_qual_map = dict(df.groupby('Neighborhood', as_index=False)['OverallQual'].mean().set_index('Neighborhood')['OverallQual'])\n",
        "df['neighborhood_qual']= df['Neighborhood'].map(neighborhood_qual_map)\n",
        "\n",
        "#Neighborhood avg Sales price:\n",
        "neighborhood_sales_map = dict(df.groupby('Neighborhood', as_index=False)['SalePrice'].mean().set_index('Neighborhood')['SalePrice'])\n",
        "df['neighborhood_avg_sale_price'] = df['Neighborhood'].map(neighborhood_sales_map)\n",
        "\n",
        "#Neighborhood avg log_Sales price:\n",
        "neighborhood_ln_sales_map = dict(df.groupby('Neighborhood', as_index=False)['ln_price'].mean().set_index('Neighborhood')['ln_price'])\n",
        "df['neighborhood_avg_ln_sale_price'] = df['Neighborhood'].map(neighborhood_ln_sales_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cgbsV7K5igH1"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 Now using the X variables that you feel like will be the best predictors of y use statsmodel to run the multiple regression between these variables and Y. You don't need to use every X variable in your dataset, in fact it's probably better if you don't. Just pick ones that you have already cleaned that seem the most relevant to house prices."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ar3WCTGTg5RZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6YR3PgK8jA8t"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.3 Identify the standard errors and P-Values of these coefficients in the output table. What is the interpretation of the P-values here?"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0XvwOWlUjPyf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6DREQUkmjQKM"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.4 Remove outliers from your dataset and run the regression again. Do you see a change in some coefficients? Which seem to move the most?"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "K6yJ9c12jXvC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YR2zFM3ajX2O"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.5 Create a new log(y) variable and use it to run a log-linear regression of your variables using statmodels "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "G3ISRRvwjwkr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PVRSM3p1jwyu"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1 Run a test for high levels of collinearity in your dataset. Calculate the Variance Inflation Factor for each X variable. Do you see VIF values greater than ten? If so try omitting those X variables and run your regression again. Do the standard errors change? Do the coefficients change? Do the coefficients seem to have an interpretation that matches your intuition?"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "I_Q9_rx6kQzM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "I7EJafYokQ9Z"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2 Variables that have high levels of multicollinearity should also be highly correlated with each other. Calculate your X matrix's correlation matrix to check if the variables highlighted by the VIF test truly are highly correlated."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sxOW6Y5EkoCG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pekJWvLzkoRu"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.3 If you have variables with high Variance Inflation Factors, try excluding them from your regression. Do your standard errors improve? (get smaller). If high levels of multicollinearity are removed, the precision of the dataset should increase."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3RsG6Fo2p1v7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AH_XQh5mp1E2"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.4 Recalculate your regression using Robust Standard Errors? What happens to your standard errors?"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JdQ3N-vRktaY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Bsq_CRqOpBSy"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.5 Use scatterplots or Seaborn's pairplot functionality to perform an eyeball test for potential variables that would be candidates for generating polynomial regressors. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jGkD_XIBpcSj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Tg3nQP3YpcxJ"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.6 Use seaborn's residplot to plot the distribution of each x variable's residuals. Does these plots indicate any other features that would be potential candidates for polynomial features."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "d7EDAAI0psaE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "W0Y0wPNAps68"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.6 Feature Engineer the appropriate polynomial features from your analysis above and include them in one final log-polynomial, robust standard error, regression. Do the coefficients of this most advanced regression match your intuition better than the coefficients of the very first regression that we ran with the Statmodels library?"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eaPcNaNaqVAj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MaNLIAtW9XDp"
      },
      "cell_type": "markdown",
      "source": [
        "# Stretch Goals\n",
        "\n",
        "- Research the assumptions that are required for OLS to be BLUE the \"Best Linear Unbiased Estimator\". You might try searching and trying to understand the conditions of what's called the Gauss-Markov Theorem.\n",
        "- Research other diagnostic tests. Can you show that residuals are normally distributed graphically?\n",
        "- Write a blog post about inferential modeling using linear regression."
      ]
    }
  ]
}